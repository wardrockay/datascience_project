{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LiYHLIwn5Irf",
        "zKwPHFZ945D6"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wardrockay/datascience_project/blob/main/Projet_Data_Science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://partage.imt.fr/index.php/s/ComZJoFowmkP5w9\n",
        "\n",
        "https://mylearningspace.imt-nord-europe.fr/pluginfile.php/69545/mod_resource/content/1/Presentation%20TP%20Data%20-%20FISA.pdf\n",
        "\n",
        "\n",
        "https://mylearningspace.imt-nord-europe.fr/pluginfile.php/69544/mod_resource/content/1/Support%20de%20TP%20Data%20mars%202024.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "IdpvndRrRrog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Repo github](https://github.com/wardrockay/datascience_project/blob/main/Projet_Data_Science.ipynb)"
      ],
      "metadata": {
        "id": "GTU5mic6Tnps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeT3QemYRa80",
        "outputId": "d0bb2709-e8bc-4438-c64d-8151aa4135a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement et fusion"
      ],
      "metadata": {
        "id": "LiYHLIwn5Irf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chargement et Fusion des Données LIBELLIUM NEW"
      ],
      "metadata": {
        "id": "5UJ_SMRfWfz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Chemin de base où les dossiers part1, part2, etc., sont stockés\n",
        "base_path = '/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/Libelium New'\n",
        "\n",
        "# Noms des fichiers à fusionner, adaptés selon vos besoins\n",
        "file_names = ['mod1.txt', 'mod2.txt']\n",
        "\n",
        "# Dictionnaire pour stocker les DataFrames concaténés\n",
        "combined_files = {}\n",
        "\n",
        "# Lire et combiner les fichiers\n",
        "for file_name in file_names:\n",
        "    frames = []  # Liste pour stocker les DataFrames de chaque partie\n",
        "    for i in range(1, 9):  # Boucler sur les dossiers de part1 à part8\n",
        "        file_path = os.path.join(base_path, f'part{i}', file_name)\n",
        "        if os.path.exists(file_path):  # Vérifier si le fichier existe\n",
        "            temp_df = pd.read_csv(file_path, delimiter='\\t', header=None)\n",
        "            temp_df[0] = pd.to_datetime(temp_df[0], errors='coerce').dt.tz_localize('UTC').dt.tz_convert('Europe/Paris')\n",
        "            # Nommer les colonnes, ajustez ces noms selon le contexte des données\n",
        "            temp_df.columns = ['Timestamp', 'Sensor1', 'Sensor2', 'Sensor3', 'Sensor4', 'Sensor5', 'Sensor6', 'Sensor7', 'Sensor8']\n",
        "            frames.append(temp_df)\n",
        "\n",
        "    # Concaténer les DataFrames de chaque fichier\n",
        "    combined_df = pd.concat(frames)\n",
        "    combined_df.sort_values('Timestamp', inplace=True)\n",
        "    combined_df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Stocker le DataFrame combiné dans le dictionnaire\n",
        "    combined_files[file_name] = combined_df\n",
        "\n",
        "# Sauvegarder les fichiers combinés\n",
        "for file_name, df in combined_files.items():\n",
        "    output_path = os.path.join(\"/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/combined/\", f'combined_{file_name}')\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"Fichiers combinés et sauvegardés avec succès.\")\n"
      ],
      "metadata": {
        "id": "QZ-LjGTQTBzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chargement et Fusion des Données PIANO\n",
        "\n"
      ],
      "metadata": {
        "id": "D1CDQWPpaH2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Définissez le chemin de base vers le dossier contenant les sous-dossiers de données\n",
        "base_path = '/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/Piano'  # Ajustez ceci selon votre configuration\n",
        "\n",
        "# Définissez les noms des fichiers à chercher dans chaque dossier\n",
        "file_names = ['IMT_PICO.csv', 'IMT_Thick.csv', 'IMT_Thin.csv']\n",
        "\n",
        "# Dictionnaire pour stocker les données combinées pour chaque type de capteur\n",
        "data_frames = {name: [] for name in file_names}\n",
        "\n",
        "for sub_folder in glob.glob(os.path.join(base_path, '*Piano')):\n",
        "    for file_name in file_names:\n",
        "        file_path = os.path.join(sub_folder, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path, delimiter=';')\n",
        "            if 'date' in df.columns:  # Check if 'date' column exists now\n",
        "                df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Convert 'date' to datetime, ignore errors\n",
        "                df.dropna(subset=['date'], inplace=True)  # Drop rows where 'date' is NaN\n",
        "                data_frames[file_name].append(df)\n",
        "            else:\n",
        "                print(f\"Date column not found in {file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# Concaténer et nettoyer les DataFrames pour chaque type de capteur\n",
        "for key, df_list in data_frames.items():\n",
        "    combined_df = pd.concat(df_list)\n",
        "    combined_df.sort_values('date', inplace=True)\n",
        "    combined_df.drop_duplicates(inplace=True)\n",
        "    data_frames[key] = combined_df\n",
        "\n",
        "# Sauvegarder les DataFrames combinés\n",
        "for sensor, df in data_frames.items():\n",
        "    output_file = os.path.join(base_path, f'combined_{sensor}')\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f'Fichier sauvegardé : {output_file}')\n",
        "\n"
      ],
      "metadata": {
        "id": "Xk7as75ZaO9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chargement et Fusion des données PODs"
      ],
      "metadata": {
        "id": "Z1fx2hwe0yIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Définissez le chemin de base vers le dossier contenant les sous-dossiers de données\n",
        "base_path = '/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/PODs'  # Ajustez ceci selon votre configuration\n",
        "\n",
        "# Définissez les noms des fichiers à chercher dans chaque dossier\n",
        "file_names = ['POD 200085.csv', 'POD 200086.csv', 'POD 200088.csv']\n",
        "\n",
        "# Dictionnaire pour stocker les données combinées pour chaque type de capteur\n",
        "data_frames = {name: [] for name in file_names}\n",
        "\n",
        "for sub_folder in glob.glob(os.path.join(base_path, '*Pods')):\n",
        "    for file_name in file_names:\n",
        "        file_path = os.path.join(sub_folder, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path, delimiter=';')\n",
        "            print(\"Columns in DataFrame after specifying delimiter:\", df.columns)  # Verify column names\n",
        "\n",
        "            if 'date' in df.columns:  # Check if 'date' column exists now\n",
        "                df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Convert 'date' to datetime, ignore errors\n",
        "                df.dropna(subset=['date'], inplace=True)  # Drop rows where 'date' is NaN\n",
        "                data_frames[file_name].append(df)\n",
        "            else:\n",
        "                print(f\"Date column not found in {file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# Concaténer et nettoyer les DataFrames pour chaque type de capteur\n",
        "for key, df_list in data_frames.items():\n",
        "    combined_df = pd.concat(df_list)\n",
        "    combined_df.sort_values('date', inplace=True)\n",
        "    combined_df.drop_duplicates(inplace=True)\n",
        "    data_frames[key] = combined_df\n",
        "\n",
        "# Sauvegarder les DataFrames combinés\n",
        "for sensor, df in data_frames.items():\n",
        "    output_file = os.path.join(base_path, f'combined_{sensor}')\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f'Fichier sauvegardé : {output_file}')\n",
        "\n"
      ],
      "metadata": {
        "id": "bKTb10iW06Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process pour sync et merge"
      ],
      "metadata": {
        "id": "MEIZVkG04w11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data"
      ],
      "metadata": {
        "id": "zKwPHFZ945D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Base path for the data files\n",
        "base_path = '/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/combined/'\n",
        "\n",
        "# Dictionary to hold the paths of each sensor's data files\n",
        "sensor_files = {\n",
        "    'libelium_new': ['combined_mod1.txt', 'combined_mod2.txt'],\n",
        "    'pods': ['combined_POD 200085.csv', 'combined_POD 200086.csv', 'combined_POD 200088.csv'],\n",
        "    'piano': ['combined_IMT_Thick.csv', 'combined_IMT_Thin.csv', 'combined_IMT_PICO.csv'],\n",
        "}\n",
        "\n",
        "def load_data(file_name, delimiter=','):\n",
        "    file_path = os.path.join(base_path, file_name)\n",
        "    if file_name.endswith('.txt'):\n",
        "        delimiter = ','  # Ensure this matches the actual file delimiter\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
        "        # Print original columns to check if they are as expected\n",
        "        print(f\"Original columns in {file_name}: {df.columns}\")\n",
        "\n",
        "        if 'Timestamp' in df.columns:\n",
        "            df.rename(columns={'Timestamp': 'timestamp'}, inplace=True)\n",
        "        elif 'date' in df.columns:\n",
        "            df.rename(columns={'date': 'timestamp'}, inplace=True)\n",
        "\n",
        "        # Convert to datetime and handle errors\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "\n",
        "        # Print columns after renaming to verify correct processing\n",
        "        print(f\"Columns after renaming in {file_name}: {df.columns}\")\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {str(e)}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "# Load all data frames\n",
        "data_frames = {key: [load_data(file) for file in files] for key, files in sensor_files.items()}\n",
        "\n",
        "for key, dfs in data_frames.items():\n",
        "    for df in dfs:\n",
        "        if not df.empty:\n",
        "            # Imprime les colonnes du DataFrame pour vérifier la présence de 'timestamp'\n",
        "            print(f\"Colonnes après le chargement ({key}): {df.columns}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DXgGS9M_43jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resample"
      ],
      "metadata": {
        "id": "ycuH9dSt76yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def resample_data(df_list, freq='10S', output_path=None):  # '10S' pour 10 secondes\n",
        "    resampled_list = []\n",
        "    for df in df_list:\n",
        "        if not df.empty:\n",
        "            # Identifier et logger les colonnes vides\n",
        "            empty_columns = df.columns[df.isna().all()]\n",
        "            if not empty_columns.empty:\n",
        "                print(f\"Colonnes supprimées car vides : {empty_columns.tolist()}\")\n",
        "\n",
        "            # Supprimer les colonnes vides\n",
        "            df.drop(empty_columns, axis=1, inplace=True)\n",
        "\n",
        "            df = df.drop_duplicates(subset='timestamp', keep='first')\n",
        "            df.set_index('timestamp', inplace=True)\n",
        "            resampled = df.resample(freq).interpolate()\n",
        "\n",
        "            # Vérifier l'unicité de l'index\n",
        "            if not resampled.index.is_unique:\n",
        "                resampled = resampled[~resampled.index.duplicated(keep='first')]\n",
        "            resampled_list.append(resampled.reset_index())\n",
        "    # Concaténer les DataFrames et les mettre dans une liste\n",
        "    combined_df = pd.concat(resampled_list, axis=0)\n",
        "\n",
        "    if output_path:\n",
        "        # Sauvegarder le DataFrame concaténé dans un fichier CSV\n",
        "        combined_df.to_csv(output_path, index=False)\n",
        "        print(f\"DataFrame saved to {output_path}\")\n",
        "\n",
        "    return [combined_df]  # Retourner le DataFrame concaténé dans une liste\n",
        "\n",
        "# Définir le chemin de base pour la sauvegarde des fichiers\n",
        "base_path = '/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/resampled/'\n",
        "\n",
        "# Rééchantillonner tous les DataFrames chargés et les sauvegarder\n",
        "resampled_data = {key: resample_data(dfs, output_path=os.path.join(base_path, f\"resampled_{key}.csv\")) for key, dfs in data_frames.items()}\n"
      ],
      "metadata": {
        "id": "Ozzr0Gg775mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge and clean"
      ],
      "metadata": {
        "id": "v9fi7f1U8Kax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import gc  # Garbage collector\n",
        "\n",
        "# Définir le chemin de base où les fichiers sont stockés\n",
        "base_path = '/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/resampled'\n",
        "\n",
        "# Fonction pour charger et nettoyer les fichiers en utilisant moins de mémoire\n",
        "def load_and_clean_data(file_path, chunksize=100000):\n",
        "    chunks = []\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
        "        # Convertir le timestamp en datetime\n",
        "        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n",
        "\n",
        "        # Imprimer les noms de colonnes pour vérifier\n",
        "        print(f\"Columns before cleanup in {file_path.split('/')[-1]}: {chunk.columns.tolist()}\")\n",
        "\n",
        "        # Supprimer les colonnes non nommées, 'element' (insensible à la casse)\n",
        "        cols_to_drop = [col for col in chunk.columns if 'Unnamed' in col or 'element' in col.strip().lower()]\n",
        "        print(f\"columns to drop: {cols_to_drop}\")\n",
        "        chunk.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "        # Supprimer les colonnes contenant 'aqi' ou 'iaq'\n",
        "        cols_to_drop_aqi_iaq = [col for col in chunk.columns if 'aqi' in col.lower() or 'iaq' in col.lower()]\n",
        "        chunk.drop(columns=cols_to_drop_aqi_iaq, inplace=True)\n",
        "\n",
        "        # Remplacer les valeurs NaN uniquement dans les colonnes numériques\n",
        "        numeric_cols = chunk.select_dtypes(include=['number']).columns\n",
        "        for col in numeric_cols:\n",
        "            if chunk[col].isnull().any():\n",
        "                chunk[col].fillna(chunk[col].median(), inplace=True)\n",
        "\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    df = pd.concat(chunks)\n",
        "    df.drop_duplicates('timestamp', inplace=True)\n",
        "\n",
        "    # Normaliser les noms de colonnes\n",
        "    df.columns = [col.lower().replace(\" \", \"_\").replace(\"-\", \"_\") for col in df.columns]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Charger les fichiers CSV dans des DataFrames en morceaux\n",
        "df_pods = load_and_clean_data(os.path.join(base_path, 'resampled_pods.csv'))\n",
        "df_piano = load_and_clean_data(os.path.join(base_path, 'resampled_piano.csv'))\n",
        "df_libelium_new = load_and_clean_data(os.path.join(base_path, 'resampled_libelium_new.csv'))\n",
        "\n",
        "# Nettoyer les objets temporaires\n",
        "gc.collect()\n",
        "\n",
        "# Modifier la fonction merge_data pour gérer directement les DataFrames\n",
        "def merge_data(dataframes):\n",
        "    merged_df = pd.DataFrame()\n",
        "    for df in dataframes:\n",
        "        if merged_df.empty:\n",
        "            merged_df = df\n",
        "        else:\n",
        "            merged_df = pd.merge(merged_df, df, on='timestamp', how='outer')\n",
        "    merged_df.drop_duplicates('timestamp', inplace=True)\n",
        "    return merged_df\n",
        "\n",
        "# Fusionner les données rééchantillonnées\n",
        "try:\n",
        "    final_merged_df = merge_data([df_pods, df_piano, df_libelium_new])\n",
        "    # Sauvegarder le DataFrame final\n",
        "    output_file_path = os.path.join(base_path, 'combined_sensors_data.csv')\n",
        "    final_merged_df.to_csv(output_file_path, index=False)\n",
        "    print(f\"Combined dataset saved as {output_file_path}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors de la fusion: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "0HxgSaZl8M6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oXvcdbvnEl0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tagger les enregistrements"
      ],
      "metadata": {
        "id": "R7URd7yqNqqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formater le calendrier des activitées"
      ],
      "metadata": {
        "id": "S3xI-U-BOFhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger les données des activités\n",
        "activities = pd.read_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/activites.csv')\n",
        "\n",
        "# Convertir les colonnes de dates en datetime avec le fuseau horaire +01:00\n",
        "activities['started'] = pd.to_datetime(activities['started'], format='%d/%m/%Y %H:%M').dt.tz_localize('Europe/Paris')\n",
        "activities['ended'] = pd.to_datetime(activities['ended'], format='%d/%m/%Y %H:%M').dt.tz_localize('Europe/Paris')\n",
        "\n",
        "# Vérifier le format des dates\n",
        "print(activities.head())\n",
        "\n",
        "# Sauvegarder le DataFrame modifié\n",
        "activities.to_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/activityAgenda.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYmCDynMOFMd",
        "outputId": "9ec85545-9e01-450a-e442-9d5688956745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  activity                   started                     ended\n",
            "0    Saber 2022-11-14 15:55:00+01:00 2022-11-14 16:42:00+01:00\n",
            "1     Aera 2022-11-14 16:42:00+01:00 2022-11-14 16:53:00+01:00\n",
            "2     Nett 2022-11-14 17:00:00+01:00 2022-11-14 17:15:00+01:00\n",
            "3      Asp 2022-11-14 17:15:00+01:00 2022-11-14 17:25:00+01:00\n",
            "4      AS1 2022-11-15 09:30:00+01:00 2022-11-15 10:00:00+01:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tagger le dataset"
      ],
      "metadata": {
        "id": "aPK6AEAPO6DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Charger les données des capteurs\n",
        "data = pd.read_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/resampled/combined_sensors_data.csv', parse_dates=['timestamp'])\n",
        "\n",
        "# Charger les données des activités\n",
        "activities = pd.read_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/activityAgenda.csv')\n",
        "activities['started'] = pd.to_datetime(activities['started'])\n",
        "activities['ended'] = pd.to_datetime(activities['ended'])\n",
        "\n",
        "# Créer une nouvelle colonne 'activity' initialisée à 'None' ou à une autre valeur par défaut\n",
        "data['activity'] = None\n",
        "\n",
        "# Associer chaque relevé à une activité\n",
        "for index, activity in activities.iterrows():\n",
        "    mask = (data['timestamp'] >= activity['started']) & (data['timestamp'] <= activity['ended'])\n",
        "    data.loc[mask, 'activity'] = activity['activity']\n",
        "\n",
        "\n",
        "print(data.head())\n",
        "\n",
        "# Enregistrer le nouveau dataset avec la nouvelle colonne\n",
        "data.to_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/resampled/updated_combined_sensors_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ckdHTZ2eNwCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generer un Graphique par activité avec une courbe par capteur"
      ],
      "metadata": {
        "id": "w0W-Ika5WbSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/resampled/updated_combined_sensors_data.csv', parse_dates=['timestamp'])\n",
        "\n",
        "# Liste des colonnes des capteurs à tracer\n",
        "sensor_columns = [col for col in data.columns if col not in ['timestamp', 'activity']]\n",
        "\n",
        "# Obtenir la liste unique des activités\n",
        "activities = data['activity'].unique()\n",
        "\n",
        "# Boucle pour créer un graphique par activité\n",
        "for activity in activities:\n",
        "    # Filtrer les données pour l'activité actuelle\n",
        "    activity_data = data[data['activity'] == activity]\n",
        "\n",
        "    # Créer une figure pour chaque activité\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title(f\"Graphique des capteurs pour l'activité : {activity}\")\n",
        "    plt.xlabel('Timestamp')\n",
        "    plt.ylabel('Valeurs des capteurs')\n",
        "\n",
        "    # Tracer chaque capteur sur le graphique\n",
        "    for sensor in sensor_columns:\n",
        "        plt.plot(activity_data['timestamp'], activity_data[sensor], label=sensor)\n",
        "\n",
        "    # Ajouter une légende\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Ajuste les ticks de l'axe des x pour une meilleure lisibilité\n",
        "    plt.gcf().autofmt_xdate()\n",
        "\n",
        "    # Sauvegarder le graphique\n",
        "    plt.savefig(f'/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/graphs/{activity}_sensors.png')\n",
        "\n",
        "    # Afficher le graphique\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DUAqKGmyWgjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "moyenne de chaque activitée"
      ],
      "metadata": {
        "id": "6mrFBtAza3D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/resampled/updated_combined_sensors_data.csv', parse_dates=['timestamp'])\n",
        "\n",
        "# Assurez-vous que 'timestamp' est l'index\n",
        "data.set_index('timestamp', inplace=True)\n",
        "\n",
        "# Liste des capteurs (colonnes numériques)\n",
        "sensor_columns = [col for col in data.columns if col not in ['activity']]\n",
        "\n",
        "# Calcul de la moyenne des colonnes numériques pour chaque ligne\n",
        "data['mean_sensors'] = data[sensor_columns].mean(axis=1)\n",
        "\n",
        "# Enregistrer le DataFrame modifié dans un fichier CSV\n",
        "data.to_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/moyenne_sensor_data.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhinJy5oa5dp",
        "outputId": "3d74d839-1212-44a1-a479-3745e27646cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-fc0b9344fe59>:5: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/resampled/updated_combined_sensors_data.csv', parse_dates=['timestamp'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot graph"
      ],
      "metadata": {
        "id": "nKIrC3AHcjdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Chemin du dossier pour sauvegarder les graphiques\n",
        "graph_dir = '/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/graphs'\n",
        "os.makedirs(graph_dir, exist_ok=True)  # Créer le dossier s'il n'existe pas\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/moyenne_sensor_data.csv', parse_dates=['timestamp'])\n",
        "data.set_index('timestamp', inplace=True)\n",
        "\n",
        "# Liste unique des activités\n",
        "activities = data['activity'].unique()\n",
        "\n",
        "# Création de graphiques pour chaque activité\n",
        "for activity in activities:\n",
        "    # Sélectionner les données pour une activité spécifique\n",
        "    activity_data = data[data['activity'] == activity]\n",
        "\n",
        "    # Tracer la courbe de la moyenne des capteurs pour l'activité\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(activity_data.index, activity_data['mean_sensors'], label=f'Moyenne des capteurs - {activity}')\n",
        "    plt.title(f'Moyenne des capteurs pour l\\'activité: {activity}')\n",
        "    plt.xlabel('Temps')\n",
        "    plt.ylabel('Moyenne des capteurs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Nom de fichier pour le graphique\n",
        "    filename = os.path.join(graph_dir, f'mean_sensors_{activity}.png')\n",
        "    plt.savefig(filename)  # Enregistrer le graphique\n",
        "    plt.close()  # Fermer la figure après l'enregistrement pour libérer la mémoire\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhNgiQ4qctj_",
        "outputId": "469017f5-bfe1-4dda-f3b9-49312a7bc426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-78f995a383c1>:10: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv('/content/drive/My Drive/IMT NORD EUROPE/2eme année/Data science/projet/moyenne_sensor_data.csv', parse_dates=['timestamp'])\n"
          ]
        }
      ]
    }
  ]
}